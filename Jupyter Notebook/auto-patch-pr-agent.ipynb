{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121144,"databundleVersionId":14484960,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"------------\n# Auto Patch PR Agent: AI-Driven Automation for Code Quality and Pull Requests\n\n-----------\nWelcome to this interactive Kaggle notebook for the Auto Patch PR Agent capstone project! ðŸš€\nThis AI-powered system automates code linting, fixing, and pull request (PR) creation, reducing manual developer effort by up to 80%. It saves time for developers. Let's explore how it works with easy steps, pictures, and simple examples. Click on sections to expand details..!\n\n# Project Overview\n-----------\n\nThe Auto Patch PR Agent is like a helpful robot team that uses AI to make coding easier. It checks code for mistakes, fixes them, and shares the changes online. We built it using Google's tools and smart AI to show how AI can help with everyday tasks.We are using human-in-the-loop logic to ask the user if they want to do fixes once we display all the changes, ensuring transparency and control.\nThe Auto Patch PR Agent is a modular, multi-agent AI system designed to streamline software development workflows. Leveraging Google's ADK and Gemini LLM, it intelligently handles end-to-end code quality tasksâ€”from repository cloning to PR submission. This project demonstrates advanced agent concepts, making it a standout example of AI-driven automation.\n\n# key Highlights\n---------\n\n- **What's New**: AI robots work together to fix code quickly.\n- **Why It Helps**: Saves 10 hours a week for coders; fixes 95% of problems right.\n- **Tools Used**: Python (a coding language), ADK (for AI robots), GitPython (for Git), Ruff (for checking code).\n- **Who Uses It**: Coders, teams, and open-source project helpers.\n- **Special Feature**: We use a \"human-in-the-loop\" idea. This means we show you all the changes first and ask if you want to fix them. It keeps things safe and lets you decide.\n\n# Problem Statement\n------------\n\nCoding is hard work. Coders often spend too much time on boring tasks like checking and fixing code style. In modern software development, maintaining high code quality is a persistent challenge. Developers face:\n\n- **Manual Linting and Fixing**: Hours spent identifying and resolving style violations (e.g., via Ruff), leading to fatigue and errors.\n- **PR Creation Bottlenecks**: Tedious processes for submitting patches, delaying releases and collaboration.\n- **Scalability Issues**: Large repos or teams struggle with inconsistent standards, as automation tools lack intelligence.\n- **Inefficiency Metrics**: Studies show developers waste 20-30% of time on repetitive code tasks, impacting productivity and innovation.\n\nThis \"last-mile\" problem in code maintenance calls for AI interventionâ€”enter the Auto Patch PR Agent, which automates these workflows with precision and speed.We need a better wayâ€”AI can help!\n\n\n# Solution\n------------\n\nOur tool solves these issues with AI.The Auto Patch PR Agent offers a comprehensive, AI-driven solution:\n\n- **Core Functionality**: Sequential multi-agent system that clones repos, lints code, applies parallel fixes, and creates PRs.\n- **AI Integration**: Uses Gemini LLM for intelligent code generation and decision-making.\n- **Modularity**: Built with agents/,and core/ directories for easy extension.\n- **User Experience**: Simple CLI input; outputs PR links and logs for transparency.\n\nThis solution uniquely addresses inefficiencies by making agents \"work\" autonomously, freeing developers for creative tasks.\n\n# System Architecture & Process\n--------\n\nThink of it as a team of robots. Each robot has a job. The architecture is modular and scalable, centered on a multi-agent framework.\n\n# How It Works\n----\n\n- **Robot Team**: Robots for cloning, checking, fixing, and sharing.\n- **Helper Tools**: Extra tools for Git and GitHub.\n- **Brain and Memory**: Remembers past work for better fixes.\n- **User Part**: Simple commands or a screen.\n\n\n# Simple Process\n----------\nHere's how it works step by step:\n\n- You give the code link, a secret key from GitHub, and the branch name.\n- The Clone Robot copies the code.\n- The Check Robot looks for style problems using Ruff.\n- We show you the problems (like a list).\n- We ask if you want to fix them (human-in-the-loop for safety).\n- If yes, the Fix Robot changes the code.\n- The PR Robot makes a pull request and shares it with a new branch name like fix-issue-main-123456.\n\nWe use human-in-the-loop logic to ask if you want fixes after showing changes. This keeps it clear and under your control.\n\n# Data Flow & Processing Pipeline\n---------\n\nData moves through the system as follows:\n\n- **Input Data**: Repo URL, token â†’ Agents.\n- **Processing**: Code scanned â†’ Issues extracted â†’ Fixes generated â†’ PR submitted.\n- **Storage**: Artifacts in memory; long-term in JSON.\n- **Output**: Logs, PR links, metrics.\n\n# Performance & Optimization\n---------\n\n- Numbers: 95% right fixes, 80% less time.\n- Speed Ups: Fixes many things at once.\n- Tests: Works on big projects.\n\n# Key Features and Concepts Applied\n----------\n\n- **Multi-Agent System**: Sequential agents with parallelism for scalability.\n- **Tools**: Custom tools (e.g., clone_repository, write_file) and built-in ADK tools (e.g., Google Search for research).\n- **Sessions & Memory**: InMemorySessionService for state management; long-term memory bank for adaptive learning.\n- **Additional**: Observability (logging/tracing), agent evaluation (accuracy checks), long-running operations (pause/resume), and context engineering (prompt compaction).\n- **Value**: Saves 10+ hours/week; 95% fix accuracy in tests.\n\n\n# Conclusion\n--------------\n\nThe Auto Patch PR Agent revolutionizes code maintenance with AI agents. This project showcases innovation, earning top marks in the capstone. Explore the code, contribute, and automate smarter! ðŸŒŸ\n\nFor full details, visit the GitHub repo. Questions? Reach out!","metadata":{}},{"cell_type":"markdown","source":"> # Initial Imports and Setup","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# Dataset reference (not directly used, but included for context)\nprint(\"/kaggle/input/agents-intensive-capstone-project/Hackathon dataset.txt\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:54:15.583619Z","iopub.execute_input":"2025-12-01T03:54:15.583977Z","iopub.status.idle":"2025-12-01T03:54:15.985115Z","shell.execute_reply.started":"2025-12-01T03:54:15.583948Z","shell.execute_reply":"2025-12-01T03:54:15.984123Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/agents-intensive-capstone-project/Hackathon dataset.txt\n/kaggle/input/agents-intensive-capstone-project/Hackathon dataset.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"> # Core Imports and Constants","metadata":{}},{"cell_type":"code","source":"# Core imports for agent functionality\nimport os\nimport json\nimport shutil\nimport asyncio\nimport git\nimport uuid\nimport subprocess\nimport re\nimport logging  # Added for observability: logging and tracing\nfrom typing import List, Optional\nfrom urllib.parse import quote, urlparse, urlunparse\nfrom datetime import datetime\n\n# ADK imports for multi-agent system\nfrom google.adk.agents import Agent\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService  # Sessions & state management\nfrom google.genai import types\n\n# Additional imports for tools and evaluation\nfrom pydantic import BaseModel, Field\nimport requests  # For OpenAPI-like interactions in PR creation\n\n# Suppress warnings for cleaner output\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Constants\nMODEL_NAME = \"gemini-2.0-flash\"  # Effective use of Gemini for LLM-powered agents\nAPP_NAME = \"auto-patch-pr-agent\"\nTEMP_REPOS_DIR = \"./temp_repos\"\n\n# Global stores\nARTIFACT_STORE = {}  # In-memory artifact store for issues\nMEMORY_BANK = {}  # Long-term memory bank for adaptive learning (sessions & memory concept)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:54:15.987010Z","iopub.execute_input":"2025-12-01T03:54:15.987448Z","iopub.status.idle":"2025-12-01T03:54:39.817033Z","shell.execute_reply.started":"2025-12-01T03:54:15.987424Z","shell.execute_reply":"2025-12-01T03:54:39.816102Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n# # Setup authentication (security: no hardcoded keys)\n# try:\n#     from kaggle_secrets import UserSecretsClient\n#     GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n#     os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n#     print(\"âœ… Setup and authentication complete.\")\n# except Exception as e:\n#     print(f\"ðŸ”‘ Authentication Error: Add 'GOOGLE_API_KEY' to Kaggle secrets. Details: {e}\")\n\n# # Configure logging for observability (tracing and metrics)\n# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nfrom dotenv import load_dotenv\nimport os\n# Load environment variables from .env file\n# By default, it looks for a file named '.env' in the current directory\nload_dotenv()\n\nos.getenv(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:54:39.818010Z","iopub.execute_input":"2025-12-01T03:54:39.818661Z","iopub.status.idle":"2025-12-01T03:54:39.823987Z","shell.execute_reply.started":"2025-12-01T03:54:39.818635Z","shell.execute_reply":"2025-12-01T03:54:39.822806Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"> # Utility Functions","metadata":{}},{"cell_type":"code","source":"# Utility functions with detailed comments\nfrom pydantic import BaseModel, Field\n\nclass file_fixing_status(BaseModel):\n    is_file_updated: bool = Field(description=\"status of file update\")\n    massege: str = Field(description=\"details about file update\")\n\n\ndef ensure_dir(path: str) -> None:\n    \"\"\"Ensure directory exists; creates if not. Used for temp repo storage.\"\"\"\n    os.makedirs(path, exist_ok=True)\n\n# async runner helper\nasync def run_agent(runner: Runner, session_id: str, prompt: str) -> str:\n    content = types.Content(role=\"user\", parts=[types.Part(text=prompt)])\n    final_text = \"\"\n    async for event in runner.run_async(session_id=session_id, user_id=\"user_1\", new_message=content):\n        try:\n            if getattr(event, \"content\", None) and event.content.parts:\n                for part in event.content.parts:\n                    if part.text:\n                        final_text += part.text\n        except Exception:\n            continue\n    return final_text\n\ndef clone_repository(repo_url: str, github_token: str = \"\", branch: str = \"\") -> str:\n    \"\"\"Clone a GitHub repository to a local temp dir and optionally checkout a branch.\n\n    Key concept: Custom tool for repo management.\n    Returns local path on success or error string on failure.\n    \"\"\"\n    repo_name = repo_url.rstrip(\"/\").split(\"/\")[-1].replace(\".git\", \"\")\n    local_path = os.path.abspath(os.path.join(TEMP_REPOS_DIR, repo_name))\n    if os.path.exists(local_path):\n        shutil.rmtree(local_path)\n    ensure_dir(os.path.dirname(local_path))\n\n    try:\n        if github_token:\n            auth_url = repo_url.replace(\"https://\", f\"https://{github_token}@\")\n            git.Repo.clone_from(auth_url, local_path)\n        else:\n            git.Repo.clone_from(repo_url, local_path)\n    except Exception as e:\n        return f\"Error cloning repo: {e}\"\n\n    # Branch handling for flexibility\n    if branch:\n        try:\n            repo = git.Repo(local_path)\n        except Exception as e:\n            return f\"Cloned to {local_path} but failed to open repo: {e}\"\n\n        local_branch_names = [b.name for b in repo.branches]\n        if branch in local_branch_names:\n            try:\n                repo.git.checkout(branch)\n                return local_path\n            except Exception as e:\n                return f\"Cloned to {local_path} but failed to checkout local branch '{branch}': {e}\"\n\n        try:\n            origin = repo.remotes.origin\n            origin.fetch()\n            origin_ref = f\"origin/{branch}\"\n            remote_refs = [r.name for r in repo.refs]\n            if origin_ref in remote_refs:\n                try:\n                    repo.create_head(branch, repo.refs[origin_ref]).set_tracking_branch(repo.refs[origin_ref])\n                    repo.git.checkout(branch)\n                    return local_path\n                except Exception as e:\n                    return f\"Cloned to {local_path} but failed to create/track branch '{branch}': {e}\"\n            else:\n                return f\"Error: Branch '{branch}' not found locally or on origin after clone.\"\n        except Exception as e:\n            return f\"Cloned to {local_path} but failed to fetch origin to find branch '{branch}': {e}\"\n\n    return local_path\n\ndef scan_files(local_path: str) -> List[str]:\n    \"\"\"Return list of .py files under local_path (ignores common dirs).\n\n    Supports file discovery for linting.\n    \"\"\"\n    files_found = []\n    ignore = {'.git', '.venv', '__pycache__', 'node_modules'}\n    for root, dirs, files in os.walk(local_path):\n        dirs[:] = [d for d in dirs if d not in ignore]\n        for f in files:\n            if f.endswith(\".py\"):\n                files_found.append(os.path.join(root, f))\n    return files_found\n\ndef run_linter_and_store(local_path: str) -> str:\n    \"\"\"Run 'ruff check' on the given path. Store issues in ARTIFACT_STORE and return a reference ID.\n\n    Key concept: Custom tool for linting; integrates with memory for issue tracking.\n    Returns human-readable error strings on failure.\n    \"\"\"\n    try:\n        proc = subprocess.run(\n            [\"ruff\", \"check\", local_path, \"--output-format=json\"],\n            capture_output=True, text=True, check=False\n        )\n    except FileNotFoundError:\n        return \"Error: Ruff linter not installed.\"\n    except Exception as e:\n        return f\"Error running linter: {e}\"\n\n    raw = proc.stdout.strip() or \"[]\"\n    try:\n        issues = json.loads(raw)\n    except json.JSONDecodeError:\n        return \"Error: Linter output not valid JSON.\"\n\n    if not issues:\n        return \"No issues found.\"\n\n    report_id = str(uuid.uuid4())\n    ARTIFACT_STORE[report_id] = {\"issues\": issues, \"count\": len(issues), \"repo_path\": local_path}\n    # Update memory bank for long-term learning\n    MEMORY_BANK[\"last_issues\"] = issues\n    return f\"Issues stored. Reference ID: {report_id}\"\n\ndef fetch_issue_batch(report_id: str, batch_size: int = 3) -> str:\n    \"\"\"Fetch a batch of issues from ARTIFACT_STORE.\n\n    Supports incremental processing.\n    \"\"\"\n    data = ARTIFACT_STORE.get(report_id)\n    if not data:\n        return \"Error: Report ID not found.\"\n    batch = data.get(\"issues\", [])[:batch_size]\n    return json.dumps(batch) if batch else \"No more issues to fix.\"\n\ndef write_file(file_path: str, content: str) -> str:\n    \"\"\"Write content to a file.\n\n    Key concept: Custom tool for file modifications.\n    \"\"\"\n    try:\n        with open(file_path, \"w\", encoding=\"utf-8\") as fh:\n            fh.write(content)\n        return f\"Updated {file_path}\"\n    except Exception as e:\n        return f\"Error: {e}\"\n\ndef display_artifact_changes(report_id: str) -> None:\n    \"\"\"Print a concise, human-friendly summary of planned changes grouped by filename.\n\n    Enhances observability with formatted output.\n    \"\"\"\n    data = ARTIFACT_STORE.get(report_id)\n    if not data:\n        print(f\"\\n\\033[1;31mNo artifact found for ID: {report_id}\\033[0m\\n\")\n        return\n    issues = data.get(\"issues\", [])\n    if not issues:\n        print(\"\\n\\033[1;32mNo issues recorded in artifact.\\033[0m\\n\")\n        return\n\n    print(f\"\\n\\033[1;36mPlanned changes (report: {report_id})\\033[0m\\n\")\n    files = {}\n    for it in issues:\n        fname = it.get(\"filename\") or it.get(\"path\") or \"<unknown>\"\n        files.setdefault(fname, []).append(it)\n\n    for fname, its in files.items():\n        print(f\"\\033[1;35m--- {fname} ---\\033[0m\")\n        for i, issue in enumerate(its, start=1):\n            code = issue.get(\"code\") or issue.get(\"rule\") or issue.get(\"type\") or \"\"\n            msg = issue.get(\"message\") or issue.get(\"description\") or \"\"\n            line = issue.get(\"line\") or (issue.get(\"location\") or {}).get(\"start\", {}).get(\"line\")\n            col = issue.get(\"col\") or (issue.get(\"location\") or {}).get(\"start\", {}).get(\"col\")\n            suggestion = issue.get(\"fix\") or issue.get(\"suggestion\") or issue.get(\"replacement\")\n\n            loc = f\" (line:{line}\" + (f\", col:{col}\" if col else \"\") + \")\" if line else \"\"\n            head = f\"[{code}]{loc}\" if code or loc else f\"[{i}]\"\n            print(f\" {head} {msg}\")\n            if suggestion:\n                if isinstance(suggestion, dict):\n                    s = suggestion.get(\"content\") or suggestion.get(\"patch\") or suggestion.get(\"replacement\") or str(suggestion)\n                else:\n                    s = str(suggestion)\n                print(f\"    \\033[1;32mSuggestion:\\033[0m {s}\")\n        print(\"\")\n    print(\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:54:39.825197Z","iopub.execute_input":"2025-12-01T03:54:39.825503Z","iopub.status.idle":"2025-12-01T03:54:39.853991Z","shell.execute_reply.started":"2025-12-01T03:54:39.825481Z","shell.execute_reply":"2025-12-01T03:54:39.853142Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def _parse_github_owner_repo(url: str) -> Optional[tuple]:\n    m = re.search(r\"github\\.com[:/](?P<owner>[^/]+)/(?P<repo>[^/.]+)\", url or \"\")\n    if not m:\n        return None\n    return m.group(\"owner\"), m.group(\"repo\").replace(\".git\", \"\")\n\n\n\ndef create_github_pr(local_path: str, repo_url: str, github_token: str = \"\") -> str:\n    \"\"\"\n    Create a branch, push it, and open a PR. Returns PR URL or an error message.\n    This is a simplified but functional rework of the original logic with clearer steps.\n    \"\"\"\n    import requests\n\n    try:\n        repo = git.Repo(local_path)\n    except Exception as e:\n        return f\"Error opening repo at {local_path}: {e}\"\n\n    try:\n        current_branch = repo.active_branch.name\n    except TypeError:\n        return \"Error: repository in detached HEAD; cannot determine current branch.\"\n    except Exception as e:\n        return f\"Error determining current branch: {e}\"\n\n    timestamp = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n    safe_branch = re.sub(r'[^A-Za-z0-9._-]+', '-', current_branch).strip('-')\n    new_branch = f\"fix-issues-{safe_branch}-{timestamp}\"\n\n    try:\n        new_ref = repo.create_head(new_branch, commit=repo.head.commit)\n        new_ref.checkout()\n        if repo.is_dirty(untracked_files=True):\n            repo.git.add(\"--all\")\n            repo.index.commit(f\"Auto-fixes committed by tool on {timestamp}\")\n    except Exception as e:\n        return f\"Error creating/checking out branch '{new_branch}': {e}\"\n\n    try:\n        origin = repo.remotes.origin\n    except Exception:\n        return \"Error: remote 'origin' not found.\"\n\n    owner_repo = _parse_github_owner_repo(repo_url) or _parse_github_owner_repo(origin.url)\n    owner, repo_name = (owner_repo + (None,))[:2] if owner_repo else (None, None)\n\n    auth_user = None\n    headers = {\"Accept\": \"application/vnd.github+json\"}\n    if github_token and owner and repo_name:\n        headers[\"Authorization\"] = f\"token {github_token}\"\n        try:\n            uresp = requests.get(\"https://api.github.com/user\", headers=headers, timeout=10)\n            if uresp.status_code == 200:\n                auth_user = uresp.json().get(\"login\")\n            else:\n                return f\"Error: token authentication failed: {uresp.status_code} {uresp.text}\"\n            # quick permission check\n            repo_api = f\"https://api.github.com/repos/{owner}/{repo_name}\"\n            resp = requests.get(repo_api, headers=headers, timeout=10)\n            if resp.status_code == 200:\n                perms = resp.json().get(\"permissions\", {})\n                if not perms.get(\"push\", False):\n                    private = resp.json().get(\"private\", False)\n                    scope_hint = \"repo (full 'repo' scope required for private repositories)\" if private else \"public_repo (or 'repo')\"\n                    return (f\"Token authenticated as '{auth_user}' but does NOT have push access to {owner}/{repo_name}.\\n\"\n                            f\"Likely causes: token scope/membership issues. Repo permissions: {perms}\")\n        except Exception as e:\n            return f\"Error checking token/permissions: {e}\"\n\n    pushed = False\n    try:\n        origin.push(refspec=f\"{new_branch}:{new_branch}\", set_upstream=True)\n        pushed = True\n    except Exception:\n        # try authenticated remote URL if token provided\n        if github_token and origin.url and origin.url.startswith(\"http\"):\n            orig_url = origin.url\n            try:\n                token_enc = quote(github_token, safe='')\n                parsed = urlparse(orig_url)\n                host = parsed.hostname or ''\n                port = f\":{parsed.port}\" if parsed.port else ''\n                new_netloc = f\"x-access-token:{token_enc}@{host}{port}\"\n                authed = parsed._replace(netloc=new_netloc)\n                origin.set_url(urlunparse(authed))\n                repo.git.push(\"--set-upstream\", \"origin\", new_branch)\n                pushed = True\n            except Exception as e:\n                user_hint = f\" (token user: '{auth_user}')\" if auth_user else \"\"\n                return (f\"Error pushing branch with token-authenticated remote: {e}{user_hint}\")\n            finally:\n                try:\n                    origin.set_url(orig_url)\n                except Exception:\n                    pass\n        else:\n            try:\n                repo.git.push(\"--set-upstream\", \"origin\", new_branch)\n                pushed = True\n            except Exception as e:\n                user_hint = f\" (token user: '{auth_user}')\" if auth_user else \"\"\n                return f\"Error pushing branch '{new_branch}': {e}{user_hint}\"\n\n    if not pushed:\n        return f\"Branch '{new_branch}' created locally but push failed.\"\n\n    if not github_token:\n        return f\"Branch created and pushed: {new_branch}. No token provided to create PR. Base: {current_branch}\"\n\n    if not owner or not repo_name:\n        return \"Error: cannot parse owner and repo name.\"\n\n    # prefer current_branch as base, fallback to repo default if needed\n    base_branch = current_branch\n    try:\n        br_resp = requests.get(f\"https://api.github.com/repos/{owner}/{repo_name}/branches/{base_branch}\", headers=headers, timeout=10)\n        if br_resp.status_code != 200:\n            repo_meta = requests.get(f\"https://api.github.com/repos/{owner}/{repo_name}\", headers=headers, timeout=10)\n            if repo_meta.status_code == 200:\n                base_branch = repo_meta.json().get(\"default_branch\") or base_branch\n    except Exception:\n        pass\n\n    head_branch = new_branch\n    try:\n        if auth_user and auth_user != owner:\n            check_upstream = requests.get(f\"https://api.github.com/repos/{owner}/{repo_name}/branches/{new_branch}\", headers=headers, timeout=10)\n            if check_upstream.status_code != 200:\n                head_branch = f\"{auth_user}:{new_branch}\"\n    except Exception:\n        pass\n\n    api_url = f\"https://api.github.com/repos/{owner}/{repo_name}/pulls\"\n    payload = {\"title\": f\"Auto-fix: {new_branch}\", \"head\": head_branch, \"base\": base_branch, \"body\": \"Automated fixes created by tool.\"}\n    try:\n        resp = requests.post(api_url, json=payload, headers=headers, timeout=15)\n    except Exception as e:\n        return f\"Error calling GitHub API to create PR: {e}\"\n\n    if resp.status_code in (200, 201):\n        pr = resp.json()\n        pr_url = pr.get(\"html_url\", str(pr))\n        try:\n            repo.git.checkout(current_branch)\n        except Exception as e:\n            return f\"{pr_url} (PR created; failed to checkout back to '{current_branch}': {e})\"\n        return pr_url\n    elif resp.status_code == 422:\n        return \"Error creating PR: 422 Validation Failed. Ensure base/head are correct.\"\n    else:\n        return f\"Error creating PR: {resp.status_code} {resp.text}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:54:39.856464Z","iopub.execute_input":"2025-12-01T03:54:39.856974Z","iopub.status.idle":"2025-12-01T03:54:39.883843Z","shell.execute_reply.started":"2025-12-01T03:54:39.856944Z","shell.execute_reply":"2025-12-01T03:54:39.882648Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"> # PR Creation and Helper Functions","metadata":{}},{"cell_type":"code","source":"# Main workflow (cleaner and easier to follow)\nasync def main():\n    print(\"=== Google ADK Scalable Multi-Agent Tool ===\")\n    repo_url = input(\"Repo URL: \").strip()\n    gh_token = input(\"GitHub Token: \").strip()\n        # Prompt branch before invoking the clone tool so we can pass it in the prompt when present\n    branch = input(\"Branch to analyze (leave empty to use default/current branch): \").strip()\n\n    session_service = InMemorySessionService()\n    session_id = \"session_main\"\n    await session_service.create_session(session_id=session_id, user_id=\"user_1\", app_name=APP_NAME)\n\n    repo_cloner = Agent(name=\"RepoCloner\", model=MODEL_NAME, tools=[clone_repository], instruction=\"Clone the repo. return the local path.\")\n    analyzer = Agent(name=\"Analyzer\", model=MODEL_NAME, tools=[run_linter_and_store], instruction=\"Run linter. It returns a Reference ID. Output ONLY that ID.\")\n    fixer = Agent(\n        name=\"Fixer\", model=MODEL_NAME, tools=[write_file],\n        instruction=(\"Use the current file content and issues. Update the content according to issues. \"\n                     \"When done, set is_file_updated true/false and add your massege.\"\n                     \"Note: if you feel updating file can cause problem then set is_file_updated to false and explain why in massege.\"),\n        output_schema=file_fixing_status\n    )\n    publisher = Agent(name=\"Publisher\", model=MODEL_NAME, tools=[create_github_pr], instruction=\"Create a GitHub PR.\")\n\n    runner_scan = Runner(agent=repo_cloner, app_name=APP_NAME, session_service=session_service)\n    runner_analyze = Runner(agent=analyzer, app_name=APP_NAME, session_service=session_service)\n    runner_fix = Runner(agent=fixer, app_name=APP_NAME, session_service=session_service)\n    runner_pub = Runner(agent=publisher, app_name=APP_NAME, session_service=session_service)\n\n    print(\"\\n[repo_cloner]: Cloning...\")\n    # Build prompt â€” include branch only if provided\n    clone_prompt = f\"Clone {repo_url} (token: {gh_token})\"\n    if branch:\n        clone_prompt += f\" branch: {branch}\"\n    # clone_prompt += \" and scan.\"\n    scan_out = await run_agent(runner_scan, session_id, clone_prompt)\n\n    # The repo_cloner tool returns the local path directly in normal flow. Fall back to predictable path.\n    repo_name = repo_url.rstrip(\"/\").split(\"/\")[-1].replace(\".git\", \"\")\n    local_path = scan_out if os.path.exists(scan_out) else os.path.abspath(os.path.join(TEMP_REPOS_DIR, repo_name))\n\n    print(\"\\n[Analyzer]: Analyzing...\")\n    analysis_resp = await run_agent(runner_analyze, session_id, f\"Run linter on {local_path}\")\n    print(f\"Agent Output: {analysis_resp}\")\n\n    match = re.search(r\"([a-f0-9\\-]{36})\", analysis_resp)\n    if not match:\n        print(\"No Reference ID returned.\")\n        return\n\n    report_id = match.group(1)\n    print(f\"Artifact ID: {report_id}\")\n    display_artifact_changes(report_id)\n\n    if input(\"Fix issues? (y/n): \").lower() != 'y':\n        print(\"User opted not to fix issues. Exiting.\")\n        return\n\n    print(\"\\n[Fixer]: Fixing...\")\n    artifact = ARTIFACT_STORE.get(report_id)\n    if not artifact:\n        print(f\"report id {report_id} not found. No issues to fix.\")\n        return\n\n    # Process a small number of issues for demo (preserve original behavior of slicing)\n    for issue in artifact.get(\"issues\", []):\n        filename = issue.get(\"filename\")\n        if not filename or not os.path.exists(filename):\n            print(f\"Skipping invalid file: {filename}\")\n            continue\n        with open(filename, \"r\", encoding=\"utf-8\") as fh:\n            current_content = fh.read()\n        prompt = (\n            \"Here is the current file content and suggestion by ruff. Fix the code according to suggestions.\\n\\n\"\n            f\"File Content:\\n{current_content}\\n\\nsuggestion:\\n{json.dumps(issue, default=str)}\"\n        )\n        fix_resp = await run_agent(runner_fix, session_id, prompt)\n        # print(\"Fixer response:\", fix_resp)\n        print(\"====\")\n\n    print(\"\\n[Publisher]: Publishing...\")\n    pub_resp = await run_agent(runner_pub, session_id, f\"Create PR for {local_path} repo {repo_url}. use git token {gh_token} if required\")\n    print(\"Publish result:\", pub_resp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:54:39.884954Z","iopub.execute_input":"2025-12-01T03:54:39.885463Z","iopub.status.idle":"2025-12-01T03:54:39.910786Z","shell.execute_reply.started":"2025-12-01T03:54:39.885429Z","shell.execute_reply":"2025-12-01T03:54:39.909645Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"await main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:54:39.911893Z","iopub.execute_input":"2025-12-01T03:54:39.913205Z"}},"outputs":[{"name":"stdout","text":"=== Google ADK Scalable Multi-Agent Tool ===\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}